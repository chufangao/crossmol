{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chufan2/miniconda3/envs/chem/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:No normalization for BCUT2D_MWHI\n",
      "WARNING:root:No normalization for BCUT2D_MWLOW\n",
      "WARNING:root:No normalization for BCUT2D_CHGHI\n",
      "WARNING:root:No normalization for BCUT2D_CHGLO\n",
      "WARNING:root:No normalization for BCUT2D_LOGPHI\n",
      "WARNING:root:No normalization for BCUT2D_LOGPLOW\n",
      "WARNING:root:No normalization for BCUT2D_MRHI\n",
      "WARNING:root:No normalization for BCUT2D_MRLOW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 1\n",
      "Splitting data with seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2039 [00:00<?, ?it/s][11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      " 20%|#9        | 398/2039 [00:00<00:00, 3964.07it/s][11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      " 40%|###9      | 808/2039 [00:00<00:00, 4043.30it/s][11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      " 59%|#####9    | 1213/2039 [00:00<00:00, 3671.28it/s][11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      " 81%|########  | 1642/2039 [00:00<00:00, 3898.57it/s][11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:10:17] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|##########| 2039/2039 [00:00<00:00, 3980.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class sizes\n",
      "p_np 0: 23.49%, 1: 76.51%\n",
      "Total size = 2,039 | train size = 1,631 | val size = 203 | test size = 205\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n"
     ]
    }
   ],
   "source": [
    "import argparse \n",
    "import sys\n",
    "sys.path.append('./grover/')\n",
    "import grover.util.parsing\n",
    "import grover.model.models\n",
    "import task.train\n",
    "\n",
    "## Use pretrain config\n",
    "parser = argparse.ArgumentParser()\n",
    "subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "                                    dest=\"parser_name\",\n",
    "                                    help=\"Subcommands for finetune, prediction, and fingerprint.\")\n",
    "parser_finetune = subparser.add_parser('finetune', help=\"Fine tune the pre-trained model.\")\n",
    "grover.util.parsing.add_finetune_args(parser_finetune)\n",
    "parser_eval = subparser.add_parser('eval', help=\"Evaluate the results of the pre-trained model.\")\n",
    "grover.util.parsing.add_finetune_args(parser_eval)\n",
    "parser_predict = subparser.add_parser('predict', help=\"Predict results from fine tuned model.\")\n",
    "grover.util.parsing.add_predict_args(parser_predict)\n",
    "parser_fp = subparser.add_parser('fingerprint', help=\"Get the fingerprints of SMILES.\")\n",
    "grover.util.parsing.add_fingerprint_args(parser_fp)\n",
    "parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "grover.util.parsing.add_pretrain_args(parser_pretrain)\n",
    "\n",
    "grover_args = parser.parse_args(\"finetune --data_path grover/exampledata/finetune/bbbp.csv \\\n",
    "                        --features_path grover/exampledata/finetune/bbbp.npz \\\n",
    "                        --save_dir grover/model/finetune/bbbp/ \\\n",
    "                        --checkpoint_path grover/model/tryout/model.ep3 \\\n",
    "                        --dataset_type classification \\\n",
    "                        --split_type scaffold_balanced \\\n",
    "                        --ensemble_size 1 \\\n",
    "                        --num_folds 3 \\\n",
    "                        --no_features_scaling \\\n",
    "                        --ffn_hidden_size 200 \\\n",
    "                        --batch_size 32 \\\n",
    "                        --epochs 10 \\\n",
    "                        --init_lr 0.00015 \\\n",
    "                        --no_cuda\".split())\n",
    "\n",
    "grover.util.parsing.modify_train_args(grover_args)\n",
    "# train_args = grover.util.parsing.get_newest_train_args()\n",
    "features_scaler, scaler, shared_dict, test_data, train_data, val_data = task.train.load_data(grover_args, print, None)\n",
    "grover_model = grover.util.utils.load_checkpoint(\"./grover/grover_large.pt\", current_args=grover_args, logger=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import grover.data\n",
    "# import torch\n",
    "\n",
    "# grover_linear_layer = torch.nn.Linear(1200, 768)\n",
    "# grover_layer = grover.model.layers.GTransEncoder(args=grover_args,\n",
    "#                                           hidden_size=768,\n",
    "#                                           edge_fdim=768,\n",
    "#                                           node_fdim=768,\n",
    "#                                           dropout=grover_args.dropout,\n",
    "#                                           activation=grover_args.activation,\n",
    "#                                           num_mt_block=1,\n",
    "#                                           num_attn_head=grover_args.num_attn_head,\n",
    "#                                           atom_emb_output=\"atom\",\n",
    "#                                           bias=grover_args.bias,\n",
    "#                                           cuda=grover_args.cuda)\n",
    "\n",
    "# mol_collator = grover.data.MolCollator(shared_dict=shared_dict, args=grover_args)\n",
    "\n",
    "# num_workers = 4\n",
    "# mol_loader = torch.utils.data.DataLoader(train_data, batch_size=grover_args.batch_size, shuffle=True,\n",
    "#                     num_workers=num_workers, collate_fn=mol_collator)\n",
    "\n",
    "# grover_model.train()\n",
    "# for item in mol_loader:\n",
    "#     _, batch, features_batch, mask, targets = item\n",
    "#     f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = batch\n",
    "#     # if next(model.parameters()).is_cuda:\n",
    "#     #     mask, targets = mask.cuda(), targets.cuda()\n",
    "\n",
    "#     # Run model\n",
    "#     grover_model.zero_grad()\n",
    "    \n",
    "#     output = grover_model.grover((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "\n",
    "#     f_atoms, f_bonds = output['atom_from_atom'], output['bond_from_atom']\n",
    "#     f_atoms = grover_linear_layer(f_atoms)\n",
    "#     f_bonds = grover_linear_layer(f_bonds)\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "    \n",
    "#     output = grover_layer((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "#     f_atoms = output[0]\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "\n",
    "#     output = grover_layer((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "#     f_atoms = output[0]\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# bert_path = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(bert_path)\n",
    "# bert_model = transformers.AutoModel.from_pretrained(bert_path)\n",
    "\n",
    "# additional_layer = transformers.models.bert.BertLayer(config=bert_model.config)\n",
    "\n",
    "# text = [\"I love drug summary\", \"I love criteria\"]\n",
    "\n",
    "# encoded_input = tokenizer(text=text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "\n",
    "# model_output  = bert_model(\n",
    "#     input_ids=encoded_input['input_ids'], \n",
    "#     token_type_ids=encoded_input['token_type_ids'], \n",
    "#     attention_mask=encoded_input['attention_mask'])\n",
    "\n",
    "# print(model_output.keys())\n",
    "# last_hidden_state = model_output['last_hidden_state']\n",
    "# extended_attention_mask = bert_model.get_extended_attention_mask(\n",
    "#     attention_mask=encoded_input['attention_mask'], \n",
    "#     input_shape=encoded_input['input_ids'].shape)\n",
    "\n",
    "# output = additional_layer(last_hidden_state, extended_attention_mask)\n",
    "# print(len(output), output[0].shape)\n",
    "\n",
    "# output = additional_layer(output[0], extended_attention_mask)\n",
    "# print(len(output), output[0].shape)\n",
    "\n",
    "# # model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "class CrossEncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, grover_args, bert_config, hidden_dim=768, nhead=8) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_layer = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True),\n",
    "            num_layers=1)\n",
    "\n",
    "        self.grover_layer = grover.model.layers.GTransEncoder(\n",
    "            args=grover_args,\n",
    "            hidden_size=hidden_dim,\n",
    "            edge_fdim=hidden_dim,\n",
    "            node_fdim=hidden_dim,\n",
    "            dropout=grover_args.dropout,\n",
    "            activation=grover_args.activation,\n",
    "            num_mt_block=1,\n",
    "            num_attn_head=grover_args.num_attn_head,\n",
    "            atom_emb_output=\"atom\",\n",
    "            bias=grover_args.bias,\n",
    "            cuda=grover_args.cuda)\n",
    "            \n",
    "        self.bert_layer = transformers.models.bert.BertLayer(config=bert_config)\n",
    "\n",
    "    def forward(self, new_text_batch, new_grover_batch):\n",
    "        last_hidden_state, extended_attention_mask = new_text_batch\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = new_grover_batch\n",
    "\n",
    "        len_split = last_hidden_state.shape[1]\n",
    "        ## last_hidden_state = (1, seq_len, 768)\n",
    "        ## f_atoms = (n_atoms, 768)\n",
    "        trans_out = self.transformer_layer(torch.cat([last_hidden_state, f_atoms.unsqueeze(dim=0)], dim=1))\n",
    "        last_hidden_state, f_atoms = trans_out[:,:len_split], trans_out[:,len_split:].squeeze()\n",
    "\n",
    "        last_hidden_state = self.bert_layer(last_hidden_state, extended_attention_mask)[0]\n",
    "        grover_out = self.grover_layer((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "        f_atoms = grover_out[0]\n",
    "\n",
    "        return (last_hidden_state, extended_attention_mask), (f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a)\n",
    "\n",
    "class CrossMol(torch.nn.Module):\n",
    "    def __init__(self, grover_args, grover_path='./grover/grover_large.pt', bert_path='dmis-lab/biobert-base-cased-v1.2', \n",
    "        num_ca_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_ca_layers = num_ca_layers\n",
    "        self.grover_args = grover_args\n",
    "        \n",
    "        self.grover_model = grover.util.utils.load_checkpoint(path=grover_path, current_args=grover_args, logger=None)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name_or_path=bert_path)\n",
    "        self.bert_model = transformers.AutoModel.from_pretrained(pretrained_model_name_or_path=bert_path)\n",
    "        self.grover_linear_layer = torch.nn.Linear(1200, 768)\n",
    "\n",
    "        self.cross_encoder_layers = torch.nn.ModuleList()\n",
    "        for i in range(self.num_ca_layers):\n",
    "            self.cross_encoder_layers.append(CrossEncoderLayer(grover_args=grover_args, bert_config=self.bert_model.config))\n",
    "\n",
    "    def forward(self, text_batch, grover_batch):\n",
    "        # ========== Initial pass through grover molecule encoder ==========\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = grover_batch\n",
    "        grover_output = self.grover_model.grover((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "        f_atoms = self.grover_linear_layer(grover_output['atom_from_atom'])\n",
    "        f_bonds = self.grover_linear_layer(grover_output['bond_from_atom'])\n",
    "        new_grover_batch = (f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a)\n",
    "\n",
    "        # ========== Initial pass through bert text encoder ==========\n",
    "        encoded_input = self.tokenizer(text=text_batch, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "        bert_output  = self.bert_model(\n",
    "            input_ids=encoded_input['input_ids'], \n",
    "            token_type_ids=encoded_input['token_type_ids'], \n",
    "            attention_mask=encoded_input['attention_mask'])\n",
    "        last_hidden_state = bert_output['last_hidden_state']\n",
    "        extended_attention_mask = self.bert_model.get_extended_attention_mask(\n",
    "            attention_mask=encoded_input['attention_mask'], \n",
    "            input_shape=encoded_input['input_ids'].shape)\n",
    "        new_text_batch = (last_hidden_state, extended_attention_mask)\n",
    "\n",
    "        # ========== Cross Attention Encoder Layers ==========\n",
    "        for layer in self.cross_encoder_layers:\n",
    "            new_text_batch, new_grover_batch = layer(new_text_batch, new_grover_batch)\n",
    "\n",
    "        return new_text_batch, new_grover_batch\n",
    "\n",
    "\n",
    "crossmol = CrossMol(grover_args=grover_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crossmol_dataset\n",
    "data_path = \"chebi20.csv\"\n",
    "batch_size = 64\n",
    "dataloader_train, dataloader_val, dataloader_test = crossmol_dataset.get_dataloaders(data_path=data_path,batch_size=1)\n",
    "\n",
    "for graph, text in dataloader_test:\n",
    "    f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = graph.get_components()\n",
    "\n",
    "    new_text_batch, new_grover_batch = crossmol.forward(text, (f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 74, 768])\n",
      "torch.Size([16, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# assumes (seq, batch, feature) as input\n",
    "\n",
    "transformer_layer = torch.nn.TransformerEncoder(\n",
    "    encoder_layer=torch.nn.TransformerEncoderLayer(d_model=768, nhead=16, batch_first=True),\n",
    "    num_layers=1)\n",
    "\n",
    "f_atoms = torch.rand(16, 768)\n",
    "last_hidden_state = torch.rand(1, 74, 768)\n",
    "len_split = last_hidden_state.shape[1]\n",
    "out = transformer_layer(torch.cat([last_hidden_state, f_atoms.unsqueeze(dim=0)], dim=1))\n",
    "\n",
    "last_hidden_state = out[:,:len_split]\n",
    "f_atom = out[:,len_split:].squeeze()\n",
    "\n",
    "print(last_hidden_state.shape)\n",
    "print(f_atom.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('chem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263b064122c2f3f5fdfaa66c498c1239ffd7fd981e94d2cad1e7e0d355d1f50c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
