{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse \n",
    "# import sys\n",
    "# sys.path.append('./grover/')\n",
    "# import grover.util.parsing\n",
    "# import grover.model.models\n",
    "# import task.train\n",
    "\n",
    "# ## Use pretrain config\n",
    "# parser = argparse.ArgumentParser()\n",
    "# subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "#                                     dest=\"parser_name\",\n",
    "#                                     help=\"Subcommands for finetune, prediction, and fingerprint.\")\n",
    "# parser_finetune = subparser.add_parser('finetune', help=\"Fine tune the pre-trained model.\")\n",
    "# grover.util.parsing.add_finetune_args(parser_finetune)\n",
    "# parser_eval = subparser.add_parser('eval', help=\"Evaluate the results of the pre-trained model.\")\n",
    "# grover.util.parsing.add_finetune_args(parser_eval)\n",
    "# parser_predict = subparser.add_parser('predict', help=\"Predict results from fine tuned model.\")\n",
    "# grover.util.parsing.add_predict_args(parser_predict)\n",
    "# parser_fp = subparser.add_parser('fingerprint', help=\"Get the fingerprints of SMILES.\")\n",
    "# grover.util.parsing.add_fingerprint_args(parser_fp)\n",
    "# parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "# grover.util.parsing.add_pretrain_args(parser_pretrain)\n",
    "\n",
    "# grover_args = parser.parse_args(\"finetune --data_path grover/exampledata/finetune/bbbp.csv \\\n",
    "#                         --features_path grover/exampledata/finetune/bbbp.npz \\\n",
    "#                         --save_dir grover/model/finetune/bbbp/ \\\n",
    "#                         --checkpoint_path grover/model/tryout/model.ep3 \\\n",
    "#                         --dataset_type classification \\\n",
    "#                         --split_type scaffold_balanced \\\n",
    "#                         --ensemble_size 1 \\\n",
    "#                         --num_folds 3 \\\n",
    "#                         --no_features_scaling \\\n",
    "#                         --ffn_hidden_size 200 \\\n",
    "#                         --batch_size 32 \\\n",
    "#                         --epochs 10 \\\n",
    "#                         --init_lr 0.00015 \\\n",
    "#                         --no_cuda\".split())\n",
    "\n",
    "# grover.util.parsing.modify_train_args(grover_args)\n",
    "# # train_args = grover.util.parsing.get_newest_train_args()\n",
    "# features_scaler, scaler, shared_dict, test_data, train_data, val_data = task.train.load_data(grover_args, print, None)\n",
    "# grover_model = grover.util.utils.load_checkpoint(\"./grover/grover_large.pt\", current_args=grover_args, logger=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import grover.data\n",
    "# import torch\n",
    "\n",
    "# grover_linear_layer = torch.nn.Linear(1200, 768)\n",
    "# grover_layer = grover.model.layers.GTransEncoder(args=grover_args,\n",
    "#                                           hidden_size=768,\n",
    "#                                           edge_fdim=768,\n",
    "#                                           node_fdim=768,\n",
    "#                                           dropout=grover_args.dropout,\n",
    "#                                           activation=grover_args.activation,\n",
    "#                                           num_mt_block=1,\n",
    "#                                           num_attn_head=grover_args.num_attn_head,\n",
    "#                                           atom_emb_output=\"atom\",\n",
    "#                                           bias=grover_args.bias,\n",
    "#                                           cuda=grover_args.cuda)\n",
    "\n",
    "# mol_collator = grover.data.MolCollator(shared_dict=shared_dict, args=grover_args)\n",
    "\n",
    "# num_workers = 4\n",
    "# mol_loader = torch.utils.data.DataLoader(train_data, batch_size=grover_args.batch_size, shuffle=True,\n",
    "#                     num_workers=num_workers, collate_fn=mol_collator)\n",
    "\n",
    "# grover_model.train()\n",
    "# for item in mol_loader:\n",
    "#     _, batch, features_batch, mask, targets = item\n",
    "#     f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = batch\n",
    "#     # if next(model.parameters()).is_cuda:\n",
    "#     #     mask, targets = mask.cuda(), targets.cuda()\n",
    "\n",
    "#     # Run model\n",
    "#     grover_model.zero_grad()\n",
    "    \n",
    "#     output = grover_model.grover((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "\n",
    "#     f_atoms, f_bonds = output['atom_from_atom'], output['bond_from_atom']\n",
    "#     f_atoms = grover_linear_layer(f_atoms)\n",
    "#     f_bonds = grover_linear_layer(f_bonds)\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "    \n",
    "#     output = grover_layer((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "#     f_atoms = output[0]\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "\n",
    "#     output = grover_layer((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "#     f_atoms = output[0]\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413, 2)\n",
      "retrieving mol from text\n",
      "Mean rank: 6.67\n",
      "Hits at 1: 0.3332323538321721\n",
      "Hits at 10: 0.8588306573765525\n",
      "Hits at 20: 0.940018176310209\n",
      "Hits at 100: 0.9957588609512269\n",
      "Hits at 500: 0.9996970614965162\n",
      "Hits at 1000: 1.0\n",
      "MRR: 0.5042599363284074\n",
      "retrieving text from mol\n",
      "Mean rank: 10.23\n",
      "Hits at 1: 0.33414116934262345\n",
      "Hits at 10: 0.8427749166919115\n",
      "Hits at 20: 0.9212359890942139\n",
      "Hits at 100: 0.9881853983641321\n",
      "Hits at 500: 0.9975764919721296\n",
      "Hits at 1000: 0.9987882459860649\n",
      "MRR: 0.5054617838749837\n"
     ]
    }
   ],
   "source": [
    "### ========== retrieval metrics ==========\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "def get_ranks(embedding1, embedding2):\n",
    "    # assumes that the true value is diagonal\n",
    "    ranks_tmp = []\n",
    "    emb = sklearn.metrics.pairwise.cosine_similarity(embedding1, embedding2) # shape: (len(embedding1), len(embedding2))\n",
    "    for k in range(emb.shape[0]):\n",
    "        cid_locs = np.argsort(emb[k])[::-1] #sort high-to-low each column\n",
    "        ranks = np.argsort(cid_locs) # get rank (original array order, but with rank instead of value)\n",
    "        ranks_tmp.append(ranks[k] + 1)\n",
    "    return np.array(ranks_tmp)\n",
    "\n",
    "def print_ranks(ranks):\n",
    "    print(\"Mean rank: {:.2f}\".format(np.mean(ranks)))\n",
    "    print(\"Hits at 1:\", np.mean(ranks <= 1))\n",
    "    print(\"Hits at 10:\", np.mean(ranks <= 10))\n",
    "    print(\"Hits at 20:\", np.mean(ranks <= 20))\n",
    "    print(\"Hits at 100:\", np.mean(ranks <= 100))\n",
    "    print(\"Hits at 500:\", np.mean(ranks <= 500))\n",
    "    print(\"Hits at 1000:\", np.mean(ranks <= 1000))\n",
    "    print(\"MRR:\", np.mean(1/ranks))\n",
    "\n",
    "# np.random.seed(seed=0)\n",
    "# e2 = np.random.random(size=(1000,768))\n",
    "# e3 = np.random.random(size=(1000,768))\n",
    "# # for i in range(len(e2)):\n",
    "# #     e2[i] += e3[i]*.1 # correlate text and mol embeddings\n",
    "\n",
    "test_output = np.load('test_output_no_ca.npy', allow_pickle=True)\n",
    "# test_output = np.load('test_output.npy', allow_pickle=True)\n",
    "print(test_output.shape)\n",
    "e1_mol = np.concatenate(test_output[:,0])\n",
    "e1_text = np.concatenate(test_output[:,1])\n",
    "# e2_text = np.concatenate(test_output[:,2])\n",
    "# e3_mol = np.concatenate(test_output[:,3])\n",
    "\n",
    "# retrieve mol from text\n",
    "print(\"retrieving mol from text\")\n",
    "ranks_tmp = get_ranks(e1_text, e1_mol)\n",
    "print_ranks(ranks_tmp)\n",
    "\n",
    "# retrieve text from mol\n",
    "print(\"retrieving text from mol\")\n",
    "ranks_tmp = get_ranks(e1_mol, e1_text)\n",
    "print_ranks(ranks_tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 768)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# use_ca = False\n",
    "# u_list = np.load('link_pred_ca={}_u_list.npy'.format(str(use_ca)))\n",
    "# v_list = np.load('link_pred_ca={}_v_list.npy'.format(str(use_ca)))\n",
    "# labels = np.load('link_pred_ca=True_labels.npy')\n",
    "\n",
    "# u_list = u_list[labels==1]\n",
    "# v_list = v_list[labels==1]\n",
    "# u_list = u_list[:1000] \n",
    "# v_list = v_list[:1000]\n",
    "# print(u_list.shape, v_list.shape, labels.shape)\n",
    "\n",
    "# ranks_tmp = get_ranks(u_list, v_list)\n",
    "# print_ranks(ranks_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# d = pd.read_csv('chebi20.csv', sep='\\t')\n",
    "# print(d.columns)\n",
    "# d['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('chem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263b064122c2f3f5fdfaa66c498c1239ffd7fd981e94d2cad1e7e0d355d1f50c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
