{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chufan2/miniconda3/envs/chem/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:No normalization for BCUT2D_MWHI\n",
      "WARNING:root:No normalization for BCUT2D_MWLOW\n",
      "WARNING:root:No normalization for BCUT2D_CHGHI\n",
      "WARNING:root:No normalization for BCUT2D_CHGLO\n",
      "WARNING:root:No normalization for BCUT2D_LOGPHI\n",
      "WARNING:root:No normalization for BCUT2D_LOGPLOW\n",
      "WARNING:root:No normalization for BCUT2D_MRHI\n",
      "WARNING:root:No normalization for BCUT2D_MRLOW\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tasks = 1\n",
      "Splitting data with seed 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2039 [00:00<?, ?it/s][13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      " 19%|#9        | 397/2039 [00:00<00:00, 3965.09it/s][13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      " 40%|###9      | 807/2039 [00:00<00:00, 4034.88it/s][13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      " 59%|#####9    | 1211/2039 [00:00<00:00, 3658.96it/s][13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      " 80%|########  | 1640/2039 [00:00<00:00, 3892.99it/s][13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:19:54] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|##########| 2039/2039 [00:00<00:00, 3968.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class sizes\n",
      "p_np 0: 23.49%, 1: 76.51%\n",
      "Total size = 2,039 | train size = 1,631 | val size = 203 | test size = 205\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n"
     ]
    }
   ],
   "source": [
    "import argparse \n",
    "import sys\n",
    "sys.path.append('./grover/')\n",
    "import grover.util.parsing\n",
    "import grover.model.models\n",
    "import task.train\n",
    "\n",
    "## Use pretrain config\n",
    "parser = argparse.ArgumentParser()\n",
    "subparser = parser.add_subparsers(title=\"subcommands\",\n",
    "                                    dest=\"parser_name\",\n",
    "                                    help=\"Subcommands for finetune, prediction, and fingerprint.\")\n",
    "parser_finetune = subparser.add_parser('finetune', help=\"Fine tune the pre-trained model.\")\n",
    "grover.util.parsing.add_finetune_args(parser_finetune)\n",
    "parser_eval = subparser.add_parser('eval', help=\"Evaluate the results of the pre-trained model.\")\n",
    "grover.util.parsing.add_finetune_args(parser_eval)\n",
    "parser_predict = subparser.add_parser('predict', help=\"Predict results from fine tuned model.\")\n",
    "grover.util.parsing.add_predict_args(parser_predict)\n",
    "parser_fp = subparser.add_parser('fingerprint', help=\"Get the fingerprints of SMILES.\")\n",
    "grover.util.parsing.add_fingerprint_args(parser_fp)\n",
    "parser_pretrain = subparser.add_parser('pretrain', help=\"Pretrain with unlabelled SMILES.\")\n",
    "grover.util.parsing.add_pretrain_args(parser_pretrain)\n",
    "\n",
    "grover_args = parser.parse_args(\"finetune --data_path grover/exampledata/finetune/bbbp.csv \\\n",
    "                        --features_path grover/exampledata/finetune/bbbp.npz \\\n",
    "                        --save_dir grover/model/finetune/bbbp/ \\\n",
    "                        --checkpoint_path grover/model/tryout/model.ep3 \\\n",
    "                        --dataset_type classification \\\n",
    "                        --split_type scaffold_balanced \\\n",
    "                        --ensemble_size 1 \\\n",
    "                        --num_folds 3 \\\n",
    "                        --no_features_scaling \\\n",
    "                        --ffn_hidden_size 200 \\\n",
    "                        --batch_size 32 \\\n",
    "                        --epochs 10 \\\n",
    "                        --init_lr 0.00015 \\\n",
    "                        --no_cuda\".split())\n",
    "\n",
    "grover.util.parsing.modify_train_args(grover_args)\n",
    "# train_args = grover.util.parsing.get_newest_train_args()\n",
    "features_scaler, scaler, shared_dict, test_data, train_data, val_data = task.train.load_data(grover_args, print, None)\n",
    "grover_model = grover.util.utils.load_checkpoint(\"./grover/grover_large.pt\", current_args=grover_args, logger=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import grover.data\n",
    "# import torch\n",
    "\n",
    "# grover_linear_layer = torch.nn.Linear(1200, 768)\n",
    "# grover_layer = grover.model.layers.GTransEncoder(args=grover_args,\n",
    "#                                           hidden_size=768,\n",
    "#                                           edge_fdim=768,\n",
    "#                                           node_fdim=768,\n",
    "#                                           dropout=grover_args.dropout,\n",
    "#                                           activation=grover_args.activation,\n",
    "#                                           num_mt_block=1,\n",
    "#                                           num_attn_head=grover_args.num_attn_head,\n",
    "#                                           atom_emb_output=\"atom\",\n",
    "#                                           bias=grover_args.bias,\n",
    "#                                           cuda=grover_args.cuda)\n",
    "\n",
    "# mol_collator = grover.data.MolCollator(shared_dict=shared_dict, args=grover_args)\n",
    "\n",
    "# num_workers = 4\n",
    "# mol_loader = torch.utils.data.DataLoader(train_data, batch_size=grover_args.batch_size, shuffle=True,\n",
    "#                     num_workers=num_workers, collate_fn=mol_collator)\n",
    "\n",
    "# grover_model.train()\n",
    "# for item in mol_loader:\n",
    "#     _, batch, features_batch, mask, targets = item\n",
    "#     f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = batch\n",
    "#     # if next(model.parameters()).is_cuda:\n",
    "#     #     mask, targets = mask.cuda(), targets.cuda()\n",
    "\n",
    "#     # Run model\n",
    "#     grover_model.zero_grad()\n",
    "    \n",
    "#     output = grover_model.grover((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "\n",
    "#     f_atoms, f_bonds = output['atom_from_atom'], output['bond_from_atom']\n",
    "#     f_atoms = grover_linear_layer(f_atoms)\n",
    "#     f_bonds = grover_linear_layer(f_bonds)\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "    \n",
    "#     output = grover_layer((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "#     f_atoms = output[0]\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "\n",
    "#     output = grover_layer((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "#     f_atoms = output[0]\n",
    "#     print(f_atoms.shape, f_bonds.shape)\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# bert_path = \"dmis-lab/biobert-base-cased-v1.2\"\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(bert_path)\n",
    "# bert_model = transformers.AutoModel.from_pretrained(bert_path)\n",
    "\n",
    "# additional_layer = transformers.models.bert.BertLayer(config=bert_model.config)\n",
    "\n",
    "# text = [\"I love drug summary\", \"I love criteria\"]\n",
    "\n",
    "# encoded_input = tokenizer(text=text, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "\n",
    "# model_output  = bert_model(\n",
    "#     input_ids=encoded_input['input_ids'], \n",
    "#     token_type_ids=encoded_input['token_type_ids'], \n",
    "#     attention_mask=encoded_input['attention_mask'])\n",
    "\n",
    "# print(model_output.keys())\n",
    "# last_hidden_state = model_output['last_hidden_state']\n",
    "# extended_attention_mask = bert_model.get_extended_attention_mask(\n",
    "#     attention_mask=encoded_input['attention_mask'], \n",
    "#     input_shape=encoded_input['input_ids'].shape)\n",
    "\n",
    "# output = additional_layer(last_hidden_state, extended_attention_mask)\n",
    "# print(len(output), output[0].shape)\n",
    "\n",
    "# output = additional_layer(output[0], extended_attention_mask)\n",
    "# print(len(output), output[0].shape)\n",
    "\n",
    "# # model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.edge_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.0.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.1.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.2.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_q.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_k.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.heads.3.mpn_v.W_h.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.layernorm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_i.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.0.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.linear_layers.2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.attn.output_linear.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.W_o.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.node_blocks.0.sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_atom_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_atom.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_1.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.W_2.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.ffn_bond_from_bond.act_func.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.atom_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_atom_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.bond_from_bond_sublayer.norm.bias\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_node.weight\".\n",
      "Loading pretrained parameter \"grover.encoders.act_func_edge.weight\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "class CrossEncoderLayer(torch.nn.Module):\n",
    "    def __init__(self, grover_args, bert_config, hidden_dim=768, nhead=8) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer_layer = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, batch_first=True),\n",
    "            num_layers=1)\n",
    "\n",
    "        self.grover_layer = grover.model.layers.GTransEncoder(\n",
    "            args=grover_args,\n",
    "            hidden_size=hidden_dim,\n",
    "            edge_fdim=hidden_dim,\n",
    "            node_fdim=hidden_dim,\n",
    "            dropout=grover_args.dropout,\n",
    "            activation=grover_args.activation,\n",
    "            num_mt_block=1,\n",
    "            num_attn_head=grover_args.num_attn_head,\n",
    "            atom_emb_output=\"atom\",\n",
    "            bias=grover_args.bias,\n",
    "            cuda=grover_args.cuda)\n",
    "            \n",
    "        self.bert_layer = transformers.models.bert.BertLayer(config=bert_config)\n",
    "\n",
    "    def forward(self, new_text_batch, new_grover_batch):\n",
    "        last_hidden_state, extended_attention_mask = new_text_batch\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = new_grover_batch\n",
    "\n",
    "        len_split = last_hidden_state.shape[1]\n",
    "        ## last_hidden_state = (1, seq_len, 768)\n",
    "        ## f_atoms = (n_atoms, 768)\n",
    "        trans_out = self.transformer_layer(torch.cat([last_hidden_state, f_atoms.unsqueeze(dim=0)], dim=1))\n",
    "        last_hidden_state, f_atoms = trans_out[:,:len_split], trans_out[:,len_split:].squeeze()\n",
    "\n",
    "        last_hidden_state = self.bert_layer(last_hidden_state, extended_attention_mask)[0]\n",
    "        grover_out = self.grover_layer((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "        f_atoms = grover_out[0]\n",
    "\n",
    "        return (last_hidden_state, extended_attention_mask), (f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a)\n",
    "\n",
    "class CrossMol(torch.nn.Module):\n",
    "    def __init__(self, grover_args, grover_path='./grover/grover_large.pt', bert_path='dmis-lab/biobert-base-cased-v1.2', \n",
    "        num_ca_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_ca_layers = num_ca_layers\n",
    "        self.grover_args = grover_args\n",
    "        \n",
    "        self.grover_model = grover.util.utils.load_checkpoint(path=grover_path, current_args=grover_args, logger=None)\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name_or_path=bert_path)\n",
    "        self.bert_model = transformers.AutoModel.from_pretrained(pretrained_model_name_or_path=bert_path)\n",
    "        self.grover_linear_layer = torch.nn.Linear(1200, 768)\n",
    "\n",
    "        self.cross_encoder_layers = torch.nn.ModuleList()\n",
    "        for i in range(self.num_ca_layers):\n",
    "            self.cross_encoder_layers.append(CrossEncoderLayer(grover_args=grover_args, bert_config=self.bert_model.config))\n",
    "\n",
    "    def forward(self, text_batch, grover_batch):\n",
    "        # ========== Initial pass through grover molecule encoder ==========\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a = grover_batch\n",
    "        grover_output = self.grover_model.grover((f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a))\n",
    "        f_atoms = self.grover_linear_layer(grover_output['atom_from_atom'])\n",
    "        f_bonds = self.grover_linear_layer(grover_output['bond_from_atom'])\n",
    "        new_grover_batch = (f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope, a2a)\n",
    "\n",
    "        # ========== Initial pass through bert text encoder ==========\n",
    "        encoded_input = self.tokenizer(text=text_batch, return_tensors='pt', truncation=True, max_length=512, padding=True)\n",
    "        bert_output  = self.bert_model(\n",
    "            input_ids=encoded_input['input_ids'], \n",
    "            token_type_ids=encoded_input['token_type_ids'], \n",
    "            attention_mask=encoded_input['attention_mask'])\n",
    "        last_hidden_state = bert_output['last_hidden_state']\n",
    "        extended_attention_mask = self.bert_model.get_extended_attention_mask(\n",
    "            attention_mask=encoded_input['attention_mask'], \n",
    "            input_shape=encoded_input['input_ids'].shape)\n",
    "        new_text_batch = (last_hidden_state, extended_attention_mask)\n",
    "\n",
    "        # ========== Cross Attention Encoder Layers ==========\n",
    "        for layer in self.cross_encoder_layers:\n",
    "            new_text_batch, new_grover_batch = layer(new_text_batch, new_grover_batch)\n",
    "\n",
    "        return new_text_batch, new_grover_batch\n",
    "\n",
    "\n",
    "crossmol = CrossMol(grover_args=grover_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413, 4)\n",
      "Mean rank: {:.2} 61.95728567100878\n",
      "Hits at 1: 0.07028173280823993\n",
      "Hits at 10: 0.32747652226598\n",
      "Hits at 20: 0.46834292638594366\n",
      "Hits at 100: 0.831869130566495\n",
      "Hits at 500: 0.9890942138745834\n",
      "Hits at 1000: 0.9954559224477431\n",
      "MRR: 0.15397264884356102\n",
      "Mean rank: {:.2} 58.93547409875795\n",
      "Hits at 1: 0.08239927294759164\n",
      "Hits at 10: 0.36746440472584063\n",
      "Hits at 20: 0.5004544077552256\n",
      "Hits at 100: 0.8433807936988791\n",
      "Hits at 500: 0.9900030293850348\n",
      "Hits at 1000: 0.9963647379581945\n",
      "MRR: 0.1735390775481024\n"
     ]
    }
   ],
   "source": [
    "### ========== retrieval metrics ==========\n",
    "import numpy as np\n",
    "import sklearn.metrics\n",
    "\n",
    "def get_ranks(embedding1, embedding2):\n",
    "    # assumes that the true value is diagonal\n",
    "    ranks_tmp = []\n",
    "    emb = sklearn.metrics.pairwise.cosine_similarity(embedding1, embedding2) # shape: (len(embedding1), len(embedding2))\n",
    "    for k in range(emb.shape[0]):\n",
    "        cid_locs = np.argsort(emb[k])[::-1] #sort high-to-low each column\n",
    "        ranks = np.argsort(cid_locs) # get rank (original array order, but with rank instead of value)\n",
    "        ranks_tmp.append(ranks[k] + 1)\n",
    "    return np.array(ranks_tmp)\n",
    "\n",
    "def print_ranks(ranks):\n",
    "    print(\"Mean rank: {:.2}\", np.mean(ranks))\n",
    "    print(\"Hits at 1:\", np.mean(ranks <= 1))\n",
    "    print(\"Hits at 10:\", np.mean(ranks <= 10))\n",
    "    print(\"Hits at 20:\", np.mean(ranks <= 20))\n",
    "    print(\"Hits at 100:\", np.mean(ranks <= 100))\n",
    "    print(\"Hits at 500:\", np.mean(ranks <= 500))\n",
    "    print(\"Hits at 1000:\", np.mean(ranks <= 1000))\n",
    "    print(\"MRR:\", np.mean(1/ranks))\n",
    "\n",
    "# np.random.seed(seed=0)\n",
    "# e2 = np.random.random(size=(1000,768))\n",
    "# e3 = np.random.random(size=(1000,768))\n",
    "# # for i in range(len(e2)):\n",
    "# #     e2[i] += e3[i]*.1 # correlate text and mol embeddings\n",
    "\n",
    "# test_output = np.load('test_output_no_ca.npy', allow_pickle=True)\n",
    "test_output = np.load('test_output.npy', allow_pickle=True)\n",
    "print(test_output.shape)\n",
    "e1_mol = np.concatenate(test_output[:,0])\n",
    "e1_text = np.concatenate(test_output[:,1])\n",
    "e2_text = np.concatenate(test_output[:,2])\n",
    "e3_mol = np.concatenate(test_output[:,3])\n",
    "\n",
    "# retrieve mol from text\n",
    "ranks_tmp = get_ranks(e2_text, e3_mol)\n",
    "print_ranks(ranks_tmp)\n",
    "\n",
    "# retrieve text from mol\n",
    "ranks_tmp = get_ranks(e3_mol, e2_text)\n",
    "print_ranks(ranks_tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1200) (1000, 1200) (230332,)\n",
      "Mean rank: {:.2} 473.425\n",
      "Hits at 1: 0.001\n",
      "Hits at 10: 0.012\n",
      "Hits at 20: 0.03\n",
      "Hits at 100: 0.143\n",
      "Hits at 500: 0.535\n",
      "Hits at 1000: 1.0\n",
      "MRR: 0.008686032770797715\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "use_ca = False\n",
    "u_list = np.load('link_pred_ca={}_u_list.npy'.format(str(use_ca)))\n",
    "v_list = np.load('link_pred_ca={}_v_list.npy'.format(str(use_ca)))\n",
    "labels = np.load('link_pred_ca=True_labels.npy')\n",
    "\n",
    "u_list = u_list[labels==1]\n",
    "v_list = v_list[labels==1]\n",
    "u_list = u_list[:1000] \n",
    "v_list = v_list[:1000]\n",
    "print(u_list.shape, v_list.shape, labels.shape)\n",
    "\n",
    "ranks_tmp = get_ranks(u_list, v_list)\n",
    "print_ranks(ranks_tmp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('chem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:58:50) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "263b064122c2f3f5fdfaa66c498c1239ffd7fd981e94d2cad1e7e0d355d1f50c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
